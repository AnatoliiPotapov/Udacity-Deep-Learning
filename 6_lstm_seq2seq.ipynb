{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "    \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reverse_sentense(sts):\n",
    "    words = sts.split()\n",
    "    sentense_rev = ' '.join([''.join(reversed(list(word))) for word in words])\n",
    "    return sentense_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id2char' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7cea969ee1c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mid2char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'id2char' is not defined"
     ]
    }
   ],
   "source": [
    "id2char(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sts = 'I will dream of you forever.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I lliw maerd fo uoy .reverof\n"
     ]
    }
   ],
   "source": [
    "def reverse_sentense(sts):\n",
    "    words = sts.split()\n",
    "    sentence_rev = ' '.join([''.join(reversed(list(word))) for word in words])\n",
    "print(sentence_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Sequence-to-sequence model with an attention mechanism.\"\"\"\n",
    "# see https://www.tensorflow.org/versions/r0.10/tutorials/seq2seq/index.html\n",
    "# compare https://github.com/tflearn/tflearn/blob/master/examples/nlp/seq2seq_example.py\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "vocab_size=256 # We are lazy, so we avoid fency mapping and just use one *class* per character/byte\n",
    "target_vocab_size=vocab_size\n",
    "learning_rate=0.1\n",
    "buckets=[(10, 10)] # our input and response words can be up to 10 characters long\n",
    "PAD=[0] # fill words shorter than 10 characters with 'padding' zeroes\n",
    "batch_size=10 # for parallel training (later)\n",
    "\n",
    "input_data    = [map(ord, \"hello\") + PAD * 5] * batch_size\n",
    "target_data   = [map(ord, \"world\") + PAD * 5] * batch_size\n",
    "target_weights= [[1.0]*6 + [0.0]*4] *batch_size # mask padding. todo: redundant --\n",
    "\n",
    "# EOS='\\n' # end of sequence symbol todo use how?\n",
    "# GO=1\t\t # start symbol 0x01 todo use how?\n",
    "\n",
    "\n",
    "class BabySeq2Seq(object):\n",
    "\n",
    "\tdef __init__(self, source_vocab_size, target_vocab_size, buckets, size, num_layers, batch_size):\n",
    "\t\tself.buckets = buckets\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.source_vocab_size = source_vocab_size\n",
    "\t\tself.target_vocab_size = target_vocab_size\n",
    "\n",
    "\t\tcell = single_cell = tf.contrib.rnn.GRUCell(size)\n",
    "\t\tif num_layers > 1:\n",
    "\t\t cell = tf.contrib.rnn.MultiRNNCell([single_cell] * num_layers)\n",
    "\n",
    "\t\t# The seq2seq function: we use embedding for the input and attention.\n",
    "\t\tdef seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "\t\t\treturn tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "\t\t\t\t\tencoder_inputs, decoder_inputs, cell,\n",
    "\t\t\t\t\tnum_encoder_symbols=source_vocab_size,\n",
    "\t\t\t\t\tnum_decoder_symbols=target_vocab_size,\n",
    "\t\t\t\t\tembedding_size=size,\n",
    "\t\t\t\t\tfeed_previous=do_decode)\n",
    "\n",
    "\t\t# Feeds for inputs.\n",
    "\t\tself.encoder_inputs = []\n",
    "\t\tself.decoder_inputs = []\n",
    "\t\tself.target_weights = []\n",
    "\t\tfor i in xrange(buckets[-1][0]):\t# Last bucket is the biggest one.\n",
    "\t\t\tself.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"encoder{0}\".format(i)))\n",
    "\t\tfor i in xrange(buckets[-1][1] + 1):\n",
    "\t\t\tself.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"decoder{0}\".format(i)))\n",
    "\t\t\tself.target_weights.append(tf.placeholder(tf.float32, shape=[None], name=\"weight{0}\".format(i)))\n",
    "\n",
    "\t\t# Our targets are decoder inputs shifted by one. OK\n",
    "\t\ttargets = [self.decoder_inputs[i + 1] for i in xrange(len(self.decoder_inputs) - 1)]\n",
    "\t\tself.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
    "\t\t\t\tself.encoder_inputs, self.decoder_inputs, targets,\n",
    "\t\t\t\tself.target_weights, buckets,\n",
    "\t\t\t\tlambda x, y: seq2seq_f(x, y, False))\n",
    "\n",
    "\t\t# Gradients update operation for training the model.\n",
    "\t\tparams = tf.trainable_variables()\n",
    "\t\tself.updates=[]\n",
    "\t\tfor b in xrange(len(buckets)):\n",
    "\t\t\tself.updates.append(tf.train.AdamOptimizer(learning_rate).minimize(self.losses[b]))\n",
    "\n",
    "\t\tself.saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "\tdef step(self, session, encoder_inputs, decoder_inputs, target_weights, test):\n",
    "\t\tbucket_id=0 # todo: auto-select\n",
    "\t\tencoder_size, decoder_size = self.buckets[bucket_id]\n",
    "\n",
    "\t\t# Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "\t\tinput_feed = {}\n",
    "\t\tfor l in xrange(encoder_size):\n",
    "\t\t\tinput_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "\t\tfor l in xrange(decoder_size):\n",
    "\t\t\tinput_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "\t\t\tinput_feed[self.target_weights[l].name] = target_weights[l]\n",
    "\n",
    "\t\t# Since our targets are decoder inputs shifted by one, we need one more.\n",
    "\t\tlast_target = self.decoder_inputs[decoder_size].name\n",
    "\t\tinput_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "\n",
    "\t\t# Output feed: depends on whether we do a backward step or not.\n",
    "\t\tif not test:\n",
    "\t\t\toutput_feed = [self.updates[bucket_id], self.losses[bucket_id]]\n",
    "\t\telse:\n",
    "\t\t\toutput_feed = [self.losses[bucket_id]]\t# Loss for this batch.\n",
    "\t\t\tfor l in xrange(decoder_size):\t# Output logits.\n",
    "\t\t\t\toutput_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "\t\toutputs = session.run(output_feed, input_feed)\n",
    "\t\tif not test:\n",
    "\t\t\treturn outputs[0], outputs[1]# Gradient norm, loss\n",
    "\t\telse:\n",
    "\t\t\treturn outputs[0], outputs[1:]# loss, outputs.\n",
    "\n",
    "\n",
    "def decode(bytes):\n",
    "\treturn \"\".join(map(chr, bytes)).replace('\\x00', '').replace('\\n', '')\n",
    "\n",
    "def test():\n",
    "\tperplexity, outputs = model.step(session, input_data, target_data, target_weights, test=True)\n",
    "\twords = np.argmax(outputs, axis=2)  # shape (10, 10, 256)\n",
    "\tword = decode(words[0])\n",
    "\tprint(\"step %d, perplexity %f, output: hello %s?\" % (step, perplexity, word))\n",
    "\tif word == \"world\":\n",
    "\t\tprint(\">>>>> success! hello \" + word + \"! <<<<<<<\")\n",
    "\t\texit()\n",
    "\n",
    "step=0\n",
    "test_step=1\n",
    "with tf.Session() as session:\n",
    "\tmodel= BabySeq2Seq(vocab_size, target_vocab_size, buckets, size=10, num_layers=1, batch_size=batch_size)\n",
    "\tsession.run(tf.global_variables_initializer())\n",
    "\twhile True:\n",
    "\t\tmodel.step(session, input_data, target_data, target_weights, test=False) # no outputs in training\n",
    "\t\tif step % test_step == 0:\n",
    "\t\t\ttest()\n",
    "\t\tstep=step+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.nn.rnn_cell.BasicRNNCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "vocab_size=256 # We are lazy, so we avoid fency mapping and just use one *class* per character/byte\n",
    "target_vocab_size=vocab_size\n",
    "learning_rate=0.1\n",
    "buckets=[(10, 10)] # our input and response words can be up to 10 characters long\n",
    "PAD=[0] # fill words shorter than 10 characters with 'padding' zeroes\n",
    "batch_size=10 # for parallel training (later)\n",
    "\n",
    "input_data    = [map(ord, \"hello\") + PAD * 5] * batch_size\n",
    "target_data   = [map(ord, \"world\") + PAD * 5] * batch_size\n",
    "target_weights= [[1.0]*6 + [0.0]*4] *batch_size # mask padding. todo: redundant --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[104, 101, 108, 108, 111, 0, 0, 0, 0, 0],\n",
       " [104, 101, 108, 108, 111, 0, 0, 0, 0, 0],\n",
       " [104, 101, 108, 108, 111, 0, 0, 0, 0, 0],\n",
       " [104, 101, 108, 108, 111, 0, 0, 0, 0, 0],\n",
       " [104, 101, 108, 108, 111, 0, 0, 0, 0, 0],\n",
       " [104, 101, 108, 108, 111, 0, 0, 0, 0, 0],\n",
       " [104, 101, 108, 108, 111, 0, 0, 0, 0, 0],\n",
       " [104, 101, 108, 108, 111, 0, 0, 0, 0, 0],\n",
       " [104, 101, 108, 108, 111, 0, 0, 0, 0, 0],\n",
       " [104, 101, 108, 108, 111, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let`s generate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
